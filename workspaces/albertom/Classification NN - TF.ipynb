{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - Import packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "from matplotlib import collections  as mc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (20.0, 10) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Load data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Train set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19423, 87)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.779214</td>\n",
       "      <td>2.016426</td>\n",
       "      <td>2.284677</td>\n",
       "      <td>0.390674</td>\n",
       "      <td>0.390674</td>\n",
       "      <td>0.390674</td>\n",
       "      <td>0.390674</td>\n",
       "      <td>6.768347</td>\n",
       "      <td>6.680881</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.833644</td>\n",
       "      <td>16.668139</td>\n",
       "      <td>16.808813</td>\n",
       "      <td>16.654894</td>\n",
       "      <td>16.755709</td>\n",
       "      <td>16.843528</td>\n",
       "      <td>14.402775</td>\n",
       "      <td>6.201567</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.015140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.634856</td>\n",
       "      <td>8.497548</td>\n",
       "      <td>8.639800</td>\n",
       "      <td>8.977254</td>\n",
       "      <td>9.014708</td>\n",
       "      <td>9.051246</td>\n",
       "      <td>9.118075</td>\n",
       "      <td>3.700733</td>\n",
       "      <td>4.616464</td>\n",
       "      <td>3.100784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.416822</td>\n",
       "      <td>5.882313</td>\n",
       "      <td>6.138657</td>\n",
       "      <td>6.594265</td>\n",
       "      <td>6.749420</td>\n",
       "      <td>6.649788</td>\n",
       "      <td>6.580524</td>\n",
       "      <td>3.295074</td>\n",
       "      <td>3.906743</td>\n",
       "      <td>3.100784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18.718844</td>\n",
       "      <td>11.035281</td>\n",
       "      <td>6.411440</td>\n",
       "      <td>3.443018</td>\n",
       "      <td>5.610607</td>\n",
       "      <td>3.743060</td>\n",
       "      <td>3.788609</td>\n",
       "      <td>6.411440</td>\n",
       "      <td>5.828984</td>\n",
       "      <td>5.610607</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3          4          5  \\\n",
       "0  12.779214   2.016426   2.284677   0.390674   0.390674   0.390674   \n",
       "1  16.833644  16.668139  16.808813  16.654894  16.755709  16.843528   \n",
       "2   9.634856   8.497548   8.639800   8.977254   9.014708   9.051246   \n",
       "3   8.416822   5.882313   6.138657   6.594265   6.749420   6.649788   \n",
       "4  18.718844  11.035281   6.411440   3.443018   5.610607   3.743060   \n",
       "\n",
       "           6         7         8         9 ...    77   78   79   80   81   82  \\\n",
       "0   0.390674  6.768347  6.680881  0.000000 ...   0.0  0.0  0.0  1.0  0.0  0.0   \n",
       "1  14.402775  6.201567  0.000000  6.015140 ...   0.0  0.0  1.0  0.0  0.0  0.0   \n",
       "2   9.118075  3.700733  4.616464  3.100784 ...   0.0  0.0  0.0  1.0  0.0  0.0   \n",
       "3   6.580524  3.295074  3.906743  3.100784 ...   0.0  0.0  0.0  1.0  0.0  0.0   \n",
       "4   3.788609  6.411440  5.828984  5.610607 ...   1.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "    83   84   85   86  \n",
       "0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 87 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_1 = pd.read_csv(\"../common/albertom/train_test_val_split/X_train.csv\", index_col=\"Unnamed: 0\")\n",
    "# df_data_1 = df_data_1.head(500)\n",
    "print(df_data_1.shape)\n",
    "df_data_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Train labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19423, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>18329863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>51718313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>32674991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>46137138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>15314372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0         1\n",
       "0  0  18329863\n",
       "1  1  51718313\n",
       "2  1  32674991\n",
       "3  1  46137138\n",
       "4  0  15314372"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target_data_1 = pd.read_csv(\"../common/albertom/train_test_val_split/y_train.csv\", index_col=\"Unnamed: 0\")\n",
    "# df_target_data_1 = df_target_data_1.head(500)\n",
    "print(df_target_data_1.shape)\n",
    "df_target_data_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - Test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4033, 87)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.634856</td>\n",
       "      <td>7.659067</td>\n",
       "      <td>8.051780</td>\n",
       "      <td>8.114748</td>\n",
       "      <td>8.146468</td>\n",
       "      <td>8.081561</td>\n",
       "      <td>8.164696</td>\n",
       "      <td>4.472104</td>\n",
       "      <td>3.100784</td>\n",
       "      <td>2.878513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17.177282</td>\n",
       "      <td>17.488188</td>\n",
       "      <td>10.715162</td>\n",
       "      <td>8.502775</td>\n",
       "      <td>8.348941</td>\n",
       "      <td>5.372040</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.906743</td>\n",
       "      <td>3.235422</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.360872</td>\n",
       "      <td>13.159441</td>\n",
       "      <td>13.101860</td>\n",
       "      <td>12.024029</td>\n",
       "      <td>12.086121</td>\n",
       "      <td>12.136122</td>\n",
       "      <td>11.940344</td>\n",
       "      <td>5.631366</td>\n",
       "      <td>4.208411</td>\n",
       "      <td>4.472104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.833644</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.475372</td>\n",
       "      <td>2.322137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.680303</td>\n",
       "      <td>2.381428</td>\n",
       "      <td>3.988994</td>\n",
       "      <td>3.480337</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.680303</td>\n",
       "      <td>2.381428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3          4          5  \\\n",
       "0   9.634856   7.659067   8.051780   8.114748   8.146468   8.081561   \n",
       "1  17.177282  17.488188  10.715162   8.502775   8.348941   5.372040   \n",
       "2  13.360872  13.159441  13.101860  12.024029  12.086121  12.136122   \n",
       "3  16.833644   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "4  16.475372   2.322137   0.000000   3.680303   2.381428   3.988994   \n",
       "\n",
       "           6         7         8         9 ...    77   78   79   80   81   82  \\\n",
       "0   8.164696  4.472104  3.100784  2.878513 ...   0.0  0.0  1.0  0.0  0.0  0.0   \n",
       "1   0.000000  0.000000  3.906743  3.235422 ...   0.0  0.0  1.0  0.0  0.0  0.0   \n",
       "2  11.940344  5.631366  4.208411  4.472104 ...   0.0  0.0  1.0  0.0  0.0  0.0   \n",
       "3   0.000000  0.000000  0.000000  0.000000 ...   1.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4   3.480337  0.000000  3.680303  2.381428 ...   0.0  1.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "    83   84   85   86  \n",
       "0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 87 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_2 = pd.read_csv(\"../common/albertom/train_test_val_split/X_test.csv\", index_col=\"Unnamed: 0\")\n",
    "# df_data_2 = df_data_2.head(100)\n",
    "print(df_data_2.shape)\n",
    "df_data_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 - Test labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4033, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10930294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>55016784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>48129536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>37805023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>22798417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0         1\n",
       "0  1  10930294\n",
       "1  1  55016784\n",
       "2  0  48129536\n",
       "3  1  37805023\n",
       "4  0  22798417"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target_data_2 = pd.read_csv(\"../common/albertom/train_test_val_split/y_test.csv\", index_col=\"Unnamed: 0\")\n",
    "# df_target_data_2 = df_target_data_2.head(100)\n",
    "print(df_target_data_2.shape)\n",
    "df_target_data_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 - Val set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3428, 87)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.634856</td>\n",
       "      <td>8.382878</td>\n",
       "      <td>8.413033</td>\n",
       "      <td>8.696147</td>\n",
       "      <td>9.017886</td>\n",
       "      <td>9.252007</td>\n",
       "      <td>9.361804</td>\n",
       "      <td>4.472104</td>\n",
       "      <td>4.707903</td>\n",
       "      <td>4.472104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17.507695</td>\n",
       "      <td>1.476656</td>\n",
       "      <td>1.574811</td>\n",
       "      <td>2.540538</td>\n",
       "      <td>1.434443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.574811</td>\n",
       "      <td>2.540538</td>\n",
       "      <td>1.434443</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.139158</td>\n",
       "      <td>11.879250</td>\n",
       "      <td>11.923661</td>\n",
       "      <td>12.013227</td>\n",
       "      <td>12.045026</td>\n",
       "      <td>11.744630</td>\n",
       "      <td>11.857166</td>\n",
       "      <td>4.000204</td>\n",
       "      <td>5.208719</td>\n",
       "      <td>3.857953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.360872</td>\n",
       "      <td>4.122501</td>\n",
       "      <td>4.122501</td>\n",
       "      <td>5.194026</td>\n",
       "      <td>4.122501</td>\n",
       "      <td>4.122501</td>\n",
       "      <td>4.122501</td>\n",
       "      <td>4.122501</td>\n",
       "      <td>5.194026</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.360872</td>\n",
       "      <td>4.332114</td>\n",
       "      <td>3.685432</td>\n",
       "      <td>2.564938</td>\n",
       "      <td>12.250356</td>\n",
       "      <td>12.022104</td>\n",
       "      <td>12.022104</td>\n",
       "      <td>3.685432</td>\n",
       "      <td>2.564938</td>\n",
       "      <td>12.250356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3          4          5  \\\n",
       "0   9.634856   8.382878   8.413033   8.696147   9.017886   9.252007   \n",
       "1  17.507695   1.476656   1.574811   2.540538   1.434443   0.000000   \n",
       "2  12.139158  11.879250  11.923661  12.013227  12.045026  11.744630   \n",
       "3  13.360872   4.122501   4.122501   5.194026   4.122501   4.122501   \n",
       "4  13.360872   4.332114   3.685432   2.564938  12.250356  12.022104   \n",
       "\n",
       "           6         7         8          9 ...    77   78   79   80   81  \\\n",
       "0   9.361804  4.472104  4.707903   4.472104 ...   0.0  0.0  1.0  0.0  0.0   \n",
       "1   0.000000  1.574811  2.540538   1.434443 ...   1.0  0.0  0.0  0.0  0.0   \n",
       "2  11.857166  4.000204  5.208719   3.857953 ...   0.0  0.0  1.0  0.0  0.0   \n",
       "3   4.122501  4.122501  5.194026   0.000000 ...   0.0  1.0  0.0  0.0  0.0   \n",
       "4  12.022104  3.685432  2.564938  12.250356 ...   0.0  0.0  1.0  0.0  0.0   \n",
       "\n",
       "    82   83   84   85   86  \n",
       "0  0.0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 87 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_3 = pd.read_csv(\"../common/albertom/train_test_val_split/X_val.csv\", index_col=\"Unnamed: 0\")\n",
    "# df_data_3 = df_data_3.head(100)\n",
    "print(df_data_3.shape)\n",
    "df_data_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 - Val labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3428, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>37382735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>16291467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>16215767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>39563885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>50610721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0         1\n",
       "0  0  37382735\n",
       "1  0  16291467\n",
       "2  0  16215767\n",
       "3  0  39563885\n",
       "4  0  50610721"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target_data_3 = pd.read_csv(\"../common/albertom/train_test_val_split/y_val.csv\", index_col=\"Unnamed: 0\")\n",
    "# df_target_data_3 = df_target_data_3.head(100)\n",
    "print(df_target_data_3.shape)\n",
    "df_target_data_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Globals:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 10001\n",
    "k_p = 0.9\n",
    "\n",
    "valid_size = df_data_3.shape[0]\n",
    "test_size = df_data_2.shape[0]\n",
    "batch_size = 128\n",
    "\n",
    "distinct_labels = len(df_target_data_1[\"0\"].unique())\n",
    "\n",
    "train_dataset = df_data_1.values\n",
    "val_dataset = df_data_3.values\n",
    "test_dataset = df_data_2.values\n",
    "\n",
    "split_labels = df_target_data_1[\"0\"].values\n",
    "train_labels = (np.arange(distinct_labels) == split_labels[:,None]).astype(np.float32)\n",
    "\n",
    "test_labels = df_target_data_2[\"0\"].values\n",
    "test_labels = (np.arange(distinct_labels) == test_labels[:,None]).astype(np.float32)\n",
    "\n",
    "val_labels = df_target_data_3[\"0\"].values\n",
    "val_labels = (np.arange(distinct_labels) == val_labels[:,None]).astype(np.float32)\n",
    "\n",
    "num_features = train_dataset.shape[1]\n",
    "num_examples = train_dataset.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Tensorflow graph: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    X = tf.placeholder(tf.float32, shape=(batch_size, num_features))\n",
    "    t = tf.placeholder(tf.int32, shape=(batch_size, distinct_labels))\n",
    "\n",
    "    L2_reg = tf.placeholder(tf.float32, shape=[])\n",
    "    learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "    \n",
    "    keep_prob = tf.constant(k_p, tf.float32)\n",
    "    \n",
    "    X_val = tf.constant(val_dataset, tf.float32)\n",
    "    X_test = tf.constant(test_dataset, tf.float32)\n",
    "\n",
    "    # Variables.\n",
    "    num_hidden1 = 30\n",
    "    num_hidden2 = 30\n",
    "    num_hidden3 = 30\n",
    "    num_hidden4 = 30\n",
    "    #num_hidden5 = 30\n",
    "    #num_hidden6 = 15\n",
    "    \n",
    "    global_step = tf.Variable(0)\n",
    "    \n",
    "    W1 = tf.Variable(tf.truncated_normal([num_features, num_hidden1]) )\n",
    "    b1 = tf.Variable(tf.zeros([num_hidden1]))\n",
    "\n",
    "    W2 = tf.Variable(tf.truncated_normal([num_hidden1, num_hidden2]))\n",
    "    b2 = tf.Variable(tf.zeros([num_hidden2]))\n",
    "    \n",
    "    W3 = tf.Variable(tf.truncated_normal([num_hidden2, num_hidden3]))\n",
    "    b3 = tf.Variable(tf.zeros([num_hidden3]))\n",
    "    \n",
    "    W4 = tf.Variable(tf.truncated_normal([num_hidden3, num_hidden4]))\n",
    "    b4 = tf.Variable(tf.zeros([num_hidden4]))\n",
    "    \n",
    "    W5 = tf.Variable(tf.truncated_normal([num_hidden4, distinct_labels]))\n",
    "    b5 = tf.Variable(tf.zeros([distinct_labels]))\n",
    "    \n",
    "    #W6 = tf.Variable(tf.truncated_normal([num_hidden5, num_hidden6]))\n",
    "    #b6 = tf.Variable(tf.zeros([num_hidden6]))\n",
    "    \n",
    "    #W7 = tf.Variable(tf.truncated_normal([num_hidden6, distinct_labels]))\n",
    "    #b7 = tf.Variable(tf.zeros([distinct_labels]))\n",
    "    \n",
    "    # Training.\n",
    "    H1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "    #H1_dropout = tf.nn.dropout(H1, keep_prob)\n",
    "    H2 = tf.nn.relu(tf.matmul(H1, W2) + b2)\n",
    "    #H2_dropout = tf.nn.dropout(H2, keep_prob)\n",
    "    H3 = tf.nn.relu(tf.matmul(H2, W3) + b3)\n",
    "    #H3_dropout = tf.nn.dropout(H3, keep_prob)\n",
    "    H4 = tf.nn.relu(tf.matmul(H3, W4) + b4)\n",
    "    #H4_dropout = tf.nn.dropout(H4, keep_prob)\n",
    "    #H5 = tf.nn.relu(tf.matmul(H4, W5) + b5)\n",
    "    #H5_dropout = tf.nn.dropout(H5, keep_prob)\n",
    "    #H6 = tf.nn.relu(tf.matmul(H5, W6) + b6)\n",
    "    #H6_dropout = tf.nn.dropout(H6, keep_prob)\n",
    "    logits = tf.matmul(H4, W5) + b5\n",
    "    \n",
    "    # Loss NO reg.\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, t))\n",
    "    regularization = (tf.nn.l2_loss(W1) + tf.nn.l2_loss(b1) + \n",
    "                      tf.nn.l2_loss(W2) + tf.nn.l2_loss(b2) + \n",
    "                      tf.nn.l2_loss(W3) + tf.nn.l2_loss(b3) +  \n",
    "                      tf.nn.l2_loss(W4) + tf.nn.l2_loss(b4) + \n",
    "                      tf.nn.l2_loss(W5) + tf.nn.l2_loss(b5))\n",
    "    \n",
    "    loss = loss + L2_reg * regularization\n",
    "\n",
    "    # Optimizer.\n",
    "    l_r = tf.train.exponential_decay(learning_rate, global_step, 100000, 0.95, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=l_r).minimize(loss)\n",
    "\n",
    "    # Predictions for training, validation.\n",
    "    train_predictions = tf.nn.softmax(logits)\n",
    "    \n",
    "    H1_val = tf.nn.relu(tf.matmul(X_val, W1) + b1)\n",
    "    H2_val = tf.nn.relu(tf.matmul(H1_val, W2) + b2)\n",
    "    H3_val = tf.nn.relu(tf.matmul(H2_val, W3) + b3)\n",
    "    H4_val = tf.nn.relu(tf.matmul(H3_val, W4) + b4)\n",
    "    #H5_val = tf.nn.relu(tf.matmul(H4_val, W5) + b5)\n",
    "    #H6_val = tf.nn.relu(tf.matmul(H5_val, W6) + b6)\n",
    "    logits_val = tf.matmul(H4_val, W5) + b5\n",
    "    \n",
    "    val_predictions = tf.nn.softmax(logits_val)\n",
    "    \n",
    "    H1_test = tf.nn.relu(tf.matmul(X_test, W1) + b1)\n",
    "    H2_test = tf.nn.relu(tf.matmul(H1_test, W2) + b2)\n",
    "    H3_test = tf.nn.relu(tf.matmul(H2_test, W3) + b3)\n",
    "    H4_test = tf.nn.relu(tf.matmul(H3_test, W4) + b4)\n",
    "    #H5_test = tf.nn.relu(tf.matmul(H4_test, W5) + b5)\n",
    "    #H6_test = tf.nn.relu(tf.matmul(H5_test, W6) + b6)\n",
    "    logits_test = tf.matmul(H4_test, W5) + b5\n",
    "    \n",
    "    test_predictions = tf.nn.softmax(logits_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Given a set of model parameters (learning rate, regularization penalty coefficient), find the optimal parameters with cross validation\n",
    "\n",
    "def holdout_validation(set_of_learning_rates, set_of_regs, X_train, y_train):\n",
    "    \n",
    "    # Get train-validation set stratified (keeps the same distribution) splitter\n",
    "    #skf = StratifiedKFold(n_splits=3)\n",
    "    set_of_params = [(x,y) for x in set_of_learning_rates for y in set_of_regs]\n",
    "    \n",
    "    #print(\"Number of folds: \" + str(n_folds))\n",
    "    print(\"Number of parameters combinations: \" + str(len(set_of_params)))\n",
    "    print(\"We will train \" + str(len(set_of_params)) + \" neural networks for this validation task.\")\n",
    "    \n",
    "    f1 = np.zeros(len(set_of_params))\n",
    "\n",
    "    t1= time.time()\n",
    "    for param_idx, param in enumerate(set_of_params):\n",
    "\n",
    "        print(str(param_idx) + \") Combination: [Parameters: \" +  str(param) + \" | Train model]\")\n",
    "        with tf.Session(graph=graph) as session:\n",
    "            tf.global_variables_initializer().run()\n",
    "                \n",
    "            for step in np.arange(num_steps):\n",
    "\n",
    "                offset = (step * batch_size) % (num_examples - batch_size)\n",
    "                X_batch = X_train[offset:(offset + batch_size), :]\n",
    "                t_batch = y_train[offset:(offset + batch_size)]\n",
    "                feed_dict = {\n",
    "                    X : X_batch,\n",
    "                    t : t_batch,\n",
    "                    L2_reg : param[1],\n",
    "                    learning_rate : param[0]\n",
    "                }\n",
    "                _, l, pred_batch = session.run( [optimizer, loss, train_predictions], feed_dict=feed_dict)\n",
    "                \n",
    "                #if (step % 500 == 0):\n",
    "                #    print(\"> Minibatch loss at step %d: %f\" % (step, l))\n",
    "                #    print(\"> Training f1: %f%%\" % score(pred_batch, t_batch))\n",
    "                #    print(\"> Validation f1: %f%%\" % score(val_predictions.eval(), val_labels))\n",
    "                \n",
    "            f1_cv = score(val_predictions.eval(), val_labels)\n",
    "            print(\"|> Final Validation f1: %.1f%%\" % f1_cv)\n",
    "            f1[param_idx] = f1[param_idx] + f1_cv\n",
    "    \n",
    "    print('>>>>>> Gridsearch done in '+ str(round((time.time()-t1)/60, 0)) + ' min')\n",
    "    print(\">>>>>> Best [learning rate - f1] = \", set_of_params[np.argmax(f1)][1])\n",
    "    print(\">>>>>> BEST [reg - f1] = \", set_of_params[np.argmax(f1)][0])\n",
    "    print(\">>>>>> SELECTED VALIDATION f1: %f%%\" % np.max(f1))\n",
    "    print(\">>>>>> THE SET OF F1 PER COMBINATION IS: \" + str(f1))\n",
    "    return set_of_params[np.argmax(f1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(predictions, labels):\n",
    "    return 100*f1_score(np.argmax(labels, 1), np.argmax(predictions, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_results(predictions, reals):\n",
    "\n",
    "    predicted_classes = np.argmax(predictions, 1)\n",
    "    real_classes = np.argmax(reals, 1)\n",
    "    \n",
    "    print(predicted_classes)\n",
    "    print(real_classes)\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.scatter(np.arange(0, len(predicted_classes)), predicted_classes, color='b')\n",
    "    plt.scatter(np.arange(0, len(real_classes)), real_classes, color='r')\n",
    "\n",
    "    lines = []\n",
    "    ax = plt.axes()\n",
    "\n",
    "    for i in range(predicted_classes.shape[0]):\n",
    "        lines.append([(i, predicted_classes[i]), (i, real_classes[i])])\n",
    "\n",
    "    lc = mc.LineCollection(lines, colors=\"g\", linewidths=0.1)\n",
    "\n",
    "    ax.add_collection(lc)\n",
    "    ax.autoscale()\n",
    "    ax.margins(0.1)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Training: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - Holdout for optimal hyperparameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - [0.0001, 0.0001] 30 30 30 30 no_dropout 128 batch size 7001 iterations -> 36,2%\n",
    " - [0.0001, 0.0012] 30 30 30 30 no_dropout 128 batch size 7001 iterations -> 38,9%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout validation on TRAINING-SET for [learning rate, L2_reg] ...\n",
      "Number of parameters combinations: 10\n",
      "We will train 10 neural networks for this validation task.\n",
      "0) Combination: [Parameters: (0.0001, 0.001) | Train model]\n",
      "|> Final Validation f1: 17.7%\n",
      "1) Combination: [Parameters: (0.0001, 0.0011000000000000001) | Train model]\n",
      "|> Final Validation f1: 3.5%\n",
      "2) Combination: [Parameters: (0.0001, 0.0012000000000000001) | Train model]\n",
      "|> Final Validation f1: 7.8%\n",
      "3) Combination: [Parameters: (0.0001, 0.0013000000000000002) | Train model]\n",
      "|> Final Validation f1: 6.5%\n",
      "4) Combination: [Parameters: (0.0001, 0.0014000000000000002) | Train model]\n",
      "|> Final Validation f1: 6.6%\n",
      "5) Combination: [Parameters: (0.0001, 0.0015000000000000002) | Train model]\n",
      "|> Final Validation f1: 12.8%\n",
      "6) Combination: [Parameters: (0.0001, 0.0016000000000000003) | Train model]\n",
      "|> Final Validation f1: 33.9%\n",
      "7) Combination: [Parameters: (0.0001, 0.0017000000000000003) | Train model]\n",
      "|> Final Validation f1: 41.0%\n",
      "8) Combination: [Parameters: (0.0001, 0.0018000000000000004) | Train model]\n",
      "|> Final Validation f1: 38.7%\n",
      "9) Combination: [Parameters: (0.0001, 0.0019000000000000004) | Train model]\n",
      "|> Final Validation f1: 1.3%\n",
      ">>>>>> Gridsearch done in 3.0 min\n",
      ">>>>>> Best [learning rate - f1] =  0.0017\n",
      ">>>>>> BEST [reg - f1] =  0.0001\n",
      ">>>>>> SELECTED VALIDATION f1: 41.018998%\n",
      ">>>>>> THE SET OF F1 PER COMBINATION IS: [ 17.66437684   3.46534653   7.80487805   6.47482014   6.60146699\n",
      "  12.78721279  33.8762215   41.01899827  38.67584399   1.30378096]\n",
      "Best parameters: [0.0001, 0.0017] ...\n"
     ]
    }
   ],
   "source": [
    "set_of_learning_rates = np.arange(0.0001, 0.0002, step=0.0001)\n",
    "set_of_L2_regs = np.arange(0.001, 0.002, step=0.0001)\n",
    "\n",
    "print(\"Holdout validation on TRAINING-SET for [learning rate, L2_reg] ...\")\n",
    "\n",
    "best_params = holdout_validation(set_of_learning_rates, set_of_L2_regs, train_dataset, train_labels)\n",
    "\n",
    "best_learning_rate = best_params[0]\n",
    "best_L2_reg = best_params[1]\n",
    "\n",
    "print(\"Best parameters: [\" + str(best_learning_rate) + \", \" + str(best_L2_reg) + \"] ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - Prediction using optimal hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for TEST-SET using best parameters ...\n",
      "Combination: [Parameters: 0.0001, 0.0019 | Train model]\n",
      "> Minibatch loss at step 0: 857.441406\n",
      "> Training f1: 40.9%\n",
      "> Validation f1: 19.6%\n",
      "> Minibatch loss at step 500: 16.346371\n",
      "> Training f1: 50.0%\n",
      "> Validation f1: 28.1%\n",
      "> Minibatch loss at step 1000: 9.537144\n",
      "> Training f1: 42.6%\n",
      "> Validation f1: 25.1%\n",
      "> Minibatch loss at step 1500: 11.200073\n",
      "> Training f1: 37.9%\n",
      "> Validation f1: 37.1%\n",
      "> Minibatch loss at step 2000: 12.663333\n",
      "> Training f1: 13.0%\n",
      "> Validation f1: 36.3%\n",
      "> Minibatch loss at step 2500: 9.826274\n",
      "> Training f1: 41.1%\n",
      "> Validation f1: 23.7%\n",
      "> Minibatch loss at step 3000: 8.992987\n",
      "> Training f1: 43.1%\n",
      "> Validation f1: 35.7%\n",
      "> Minibatch loss at step 3500: 8.020615\n",
      "> Training f1: 27.7%\n",
      "> Validation f1: 18.6%\n",
      "> Minibatch loss at step 4000: 14.079302\n",
      "> Training f1: 6.5%\n",
      "> Validation f1: 26.2%\n",
      "> Minibatch loss at step 4500: 6.564208\n",
      "> Training f1: 37.9%\n",
      "> Validation f1: 39.2%\n",
      "> Minibatch loss at step 5000: 6.837103\n",
      "> Training f1: 35.1%\n",
      "> Validation f1: 36.7%\n",
      "> Minibatch loss at step 5500: 6.538415\n",
      "> Training f1: 8.3%\n",
      "> Validation f1: 29.6%\n",
      "> Minibatch loss at step 6000: 7.484358\n",
      "> Training f1: 45.6%\n",
      "> Validation f1: 39.2%\n",
      "> Minibatch loss at step 6500: 6.381654\n",
      "> Training f1: 36.4%\n",
      "> Validation f1: 15.9%\n",
      "> Minibatch loss at step 7000: 6.162222\n",
      "> Training f1: 47.2%\n",
      "> Validation f1: 19.6%\n",
      "|> Test f1: 20.018198%\n",
      "|> Test accuracy: 78.204810%\n",
      "[0 0 0 ..., 0 0 0]\n",
      "[1 1 0 ..., 1 0 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIQAAAJCCAYAAACxsxylAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAA6IyEEAAAA0BkJIQAAAIDO/H9AkShssUu+nAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a384c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Prediction for TEST-SET using best parameters ...\")\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Combination: [Parameters: \" +  str(best_learning_rate) + \", \" + str(best_L2_reg) + \" | Train model]\")\n",
    "    for step in np.arange(num_steps):\n",
    "        \n",
    "                        \n",
    "        offset = (step * batch_size) % (num_examples - batch_size)\n",
    "        X_batch = train_dataset[offset:(offset + batch_size), :]\n",
    "        t_batch = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {\n",
    "            X : X_batch,\n",
    "            t : t_batch,\n",
    "            L2_reg : best_L2_reg,\n",
    "            learning_rate : best_learning_rate\n",
    "        }\n",
    "        \n",
    "        _, l, pred_batch = session.run( [optimizer, loss, train_predictions], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"> Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"> Training f1: %.1f%%\" % score(pred_batch, t_batch))\n",
    "            print(\"> Validation f1: %.1f%%\" % score(val_predictions.eval(), val_labels))\n",
    "                \n",
    "    f1_test = score(test_predictions.eval(), test_labels)\n",
    "    accuracy_test = accuracy(test_predictions.eval(), test_labels)\n",
    "    print(\"|> Test f1: %f%%\" % f1_test)\n",
    "    print(\"|> Test accuracy: %f%%\" % accuracy_test)\n",
    "    plot_results(test_predictions.eval(), test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
